{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e025829-1a06-4ed3-b1b0-75f07d353fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "# Load configuration from a YAML file\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "# Define the main function for encoding images\n",
    "def encode_images(config_path):\n",
    "    # Load configuration\n",
    "    config = load_config(config_path)\n",
    "    dataset_dir = config['dataset_dir']\n",
    "    encodings_dir = config['encodings_dir']\n",
    "    batch_size = config.get('batch_size', 32)\n",
    "    excluded_models = config.get('excluded_models', [])\n",
    "\n",
    "    # Set the device to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Expanded list of pre-trained models to 70 models\n",
    "    model_names = config['available_models']\n",
    "\n",
    "    # Exclude specified models\n",
    "    excluded_models = config.get('excluded_models', [])\n",
    "    model_names = [model for model in model_names if model not in excluded_models]\n",
    "\n",
    "    # Define the local directory for storing models and temporary files\n",
    "    local_model_dir = config.get('local_model_dir', \"pretrained_models\")\n",
    "    os.makedirs(local_model_dir, exist_ok=True)\n",
    "    torch.hub.set_dir(local_model_dir)  # Set PyTorch to use the specified local directory for model storage\n",
    "\n",
    "    # Define a transform to resize images and convert them to tensors\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create the 'encodings' directory if it does not exist\n",
    "    os.makedirs(encodings_dir, exist_ok=True)\n",
    "\n",
    "    # Custom loader function to handle loading errors\n",
    "    def pil_loader_with_error_handling(path: str):\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                img = Image.open(f)\n",
    "                return img.convert(\"RGB\")\n",
    "        except UnidentifiedImageError:\n",
    "            print(f\"Skipping file {path}: cannot identify image file.\")\n",
    "            return None\n",
    "\n",
    "    # Load the dataset using ImageFolder with the custom loader\n",
    "    dataset = ImageFolder(root=dataset_dir, transform=transform, loader=pil_loader_with_error_handling)\n",
    "\n",
    "    # Filter out invalid images\n",
    "    valid_indices = [i for i, (img, _) in enumerate(dataset.imgs) if dataset.loader(dataset.imgs[i][0]) is not None]\n",
    "    valid_dataset = torch.utils.data.Subset(dataset, valid_indices)\n",
    "    dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Display the number of images to be encoded\n",
    "    num_images = len(valid_dataset)\n",
    "    print(f\"Number of images to be encoded: {num_images}\")\n",
    "\n",
    "    # Loop through each model and encode images\n",
    "    latest_model = sorted(model_names, key=lambda model: os.path.getctime(os.path.join(encodings_dir, model)) if os.path.exists(os.path.join(encodings_dir, model)) else 0, reverse=True)[0]\n",
    "    for model_name in model_names:\n",
    "        if model_name == latest_model:\n",
    "            print(f\"Resuming with most recently incomplete model: {model_name}\")\n",
    "        else:\n",
    "            print(f\"Processing with model: {model_name}\")\n",
    "\n",
    "        model_encodings_dir = os.path.join(encodings_dir, model_name)\n",
    "        os.makedirs(model_encodings_dir, exist_ok=True)\n",
    "\n",
    "        # Check if the encodings for this model already exist or if there are incomplete encodings\n",
    "        if encodings_exist(model_encodings_dir) and not os.path.exists(os.path.join(model_encodings_dir, 'features_temp.npy')):\n",
    "            print(f\"Encodings for model {model_name} already exist and are complete. Skipping to next model.\")\n",
    "            continue\n",
    "\n",
    "        # Load or download model\n",
    "        model = load_or_download_model(model_name, local_model_dir, device)\n",
    "\n",
    "        # Modify the model to use only the feature extractor part\n",
    "        model = modify_model_for_feature_extraction(model, model_name)\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Try to load saved batches if any exist\n",
    "        loaded_features, loaded_labels, last_batch_index = load_saved_batches(model_encodings_dir)\n",
    "        # Initialize last_batch_index to 0 if no saved batches are available\n",
    "        if last_batch_index is None:\n",
    "            last_batch_index = 0\n",
    "\n",
    "        # If there are saved batches, resume from where it left off        \n",
    "        if loaded_features is not None and loaded_labels is not None:\n",
    "            all_features = list(loaded_features)\n",
    "            all_labels = list(loaded_labels)\n",
    "            print(f\"Resuming from saved batch for model: {model_name}\")\n",
    "\n",
    "        for batch_index, (images, labels) in enumerate(tqdm(dataloader)):\n",
    "            # Skip batches that have already been processed\n",
    "            if batch_index < last_batch_index:\n",
    "                continue\n",
    "\n",
    "            images = images.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(images)\n",
    "                output = process_output(output, model_name)\n",
    "\n",
    "            features = output.cpu().numpy()\n",
    "            all_features.append(features)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "            # Save temporary batches after each iteration\n",
    "            last_batch_index = batch_index + 1\n",
    "            save_temp_batches(model_encodings_dir, np.array(all_features, dtype=object), np.array(all_labels, dtype=object), last_batch_index)\n",
    "\n",
    "        all_features = np.concatenate(all_features, axis=0)\n",
    "        all_labels = np.array(all_labels)\n",
    "\n",
    "        # Display the dimensions of all_features and all_labels\n",
    "        print(f\"Dimensions of all_features: {all_features.shape}\")\n",
    "        print(f\"Dimensions of all_labels: {all_labels.shape}\")\n",
    "\n",
    "        # Save the encoded images and labels as .npy files\n",
    "        np.save(os.path.join(model_encodings_dir, 'encoded_images.npy'), all_features)\n",
    "        np.save(os.path.join(model_encodings_dir, 'labels.npy'), all_labels)\n",
    "\n",
    "        # Delete temporary batches after completion if encoding is successful\n",
    "        delete_temp_batches(model_encodings_dir)\n",
    "        # Remove last batch index file if encoding is successful\n",
    "        last_batch_index_path = os.path.join(model_encodings_dir, 'last_batch_index.npy')\n",
    "        if os.path.exists(last_batch_index_path):\n",
    "            os.remove(last_batch_index_path)\n",
    "\n",
    "        print(f\"Encoding and saving complete for model: {model_name}\")\n",
    "\n",
    "    print(\"All models processed!\")\n",
    "\n",
    "# Helper functions for loading, processing, and saving data\n",
    "def load_or_download_model(model_name, local_model_dir, device):\n",
    "    checkpoints_dir = os.path.join(local_model_dir, 'checkpoints')\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoints_dir, f\"{model_name}.pth\")\n",
    "\n",
    "    # Attempt to find existing model checkpoint by pattern matching\n",
    "    existing_checkpoints = glob.glob(os.path.join(checkpoints_dir, f\"{model_name}*.pth\"))\n",
    "\n",
    "    if existing_checkpoints:\n",
    "        print(f\"Found existing model weights for {model_name}. Loading from local directory.\")\n",
    "        model = getattr(models, model_name)(weights=None)\n",
    "        model.load_state_dict(torch.load(existing_checkpoints[0]))\n",
    "    else:\n",
    "        print(f\"Model weights for {model_name} not found. Downloading...\")\n",
    "        model = getattr(models, model_name)(weights='DEFAULT')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "    return model.to(device)\n",
    "\n",
    "def modify_model_for_feature_extraction(model, model_name):\n",
    "    # Modify the model to use only the feature extractor part\n",
    "    if 'resnet' in model_name or 'resnext' in model_name or 'wide_resnet' in model_name:\n",
    "        model = torch.nn.Sequential(*list(model.children())[:-2])\n",
    "    elif 'mobilenet' in model_name or 'mnasnet' in model_name:\n",
    "        model.classifier = torch.nn.Identity()\n",
    "    elif 'vgg' in model_name or 'alexnet' in model_name:\n",
    "        model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "    elif 'squeezenet' in model_name:\n",
    "        model.classifier = torch.nn.Identity()\n",
    "    elif 'googlenet' in model_name or 'inception' in model_name:\n",
    "        model.fc = torch.nn.Identity()\n",
    "    elif 'efficientnet' in model_name or 'convnext' in model_name:\n",
    "        model.classifier = torch.nn.Identity()\n",
    "    elif 'shufflenet' in model_name or 'regnet' in model_name:\n",
    "        model.fc = torch.nn.Identity()  # Removing the classifier head for feature extraction\n",
    "    elif 'densenet' in model_name:\n",
    "        model.classifier = torch.nn.Identity()\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} is not supported for feature extraction.\")\n",
    "    return model\n",
    "\n",
    "def model_weights_exist(model_name, local_model_dir):\n",
    "    checkpoints_dir = os.path.join(local_model_dir, 'checkpoints')\n",
    "    return len(glob.glob(os.path.join(checkpoints_dir, f\"{model_name}*.pth\"))) > 0\n",
    "\n",
    "def encodings_exist(model_encodings_dir):\n",
    "    features_path = os.path.join(model_encodings_dir, 'encoded_images.npy')\n",
    "    labels_path = os.path.join(model_encodings_dir, 'labels.npy')\n",
    "    return os.path.exists(features_path) and os.path.exists(labels_path)\n",
    "\n",
    "def load_saved_batches(model_encodings_dir):\n",
    "    temp_features_path = os.path.join(model_encodings_dir, 'features_temp.npy')\n",
    "    temp_labels_path = os.path.join(model_encodings_dir, 'labels_temp.npy')\n",
    "    last_batch_index_path = os.path.join(model_encodings_dir, 'last_batch_index.npy')\n",
    "    if os.path.exists(temp_features_path) and os.path.exists(temp_labels_path) and os.path.exists(last_batch_index_path):\n",
    "        try:\n",
    "            features = np.load(temp_features_path, allow_pickle=True)\n",
    "            labels = np.load(temp_labels_path, allow_pickle=True)\n",
    "            last_batch_index = np.load(last_batch_index_path, allow_pickle=True)\n",
    "            return features, labels, last_batch_index\n",
    "        except (EOFError, ValueError):\n",
    "            print(f\"Error loading temporary batches for {model_encodings_dir}. Deleting corrupted files and restarting from last known good state.\")\n",
    "            # Delete corrupted files to allow fresh restart\n",
    "            if os.path.exists(temp_features_path):\n",
    "                os.remove(temp_features_path)\n",
    "            if os.path.exists(temp_labels_path):\n",
    "                os.remove(temp_labels_path)\n",
    "            if os.path.exists(last_batch_index_path):\n",
    "                os.remove(last_batch_index_path)\n",
    "    return None, None, None\n",
    "\n",
    "def save_temp_batches(model_encodings_dir, features, labels, last_batch_index):\n",
    "    temp_features_path = os.path.join(model_encodings_dir, 'features_temp.npy')\n",
    "    temp_labels_path = os.path.join(model_encodings_dir, 'labels_temp.npy')\n",
    "    last_batch_index_path = os.path.join(model_encodings_dir, 'last_batch_index.npy')\n",
    "    np.save(temp_features_path, features, allow_pickle=True)\n",
    "    np.save(temp_labels_path, labels, allow_pickle=True)\n",
    "    np.save(last_batch_index_path, last_batch_index, allow_pickle=True)\n",
    "\n",
    "def delete_temp_batches(model_encodings_dir):\n",
    "    temp_features_path = os.path.join(model_encodings_dir, 'features_temp.npy')\n",
    "    temp_labels_path = os.path.join(model_encodings_dir, 'labels_temp.npy')\n",
    "    last_batch_index_path = os.path.join(model_encodings_dir, 'last_batch_index.npy')\n",
    "    if os.path.exists(temp_features_path):\n",
    "        os.remove(temp_features_path)\n",
    "    if os.path.exists(temp_labels_path):\n",
    "        os.remove(temp_labels_path)\n",
    "    if os.path.exists(last_batch_index_path):\n",
    "        os.remove(last_batch_index_path)\n",
    "\n",
    "def process_output(output, model_name):\n",
    "    global_avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "    if len(output.size()) == 4:\n",
    "        output = global_avg_pool(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "    elif len(output.size()) != 2:\n",
    "        raise ValueError(f\"Unexpected output size from model {model_name}: {output.size()}\")\n",
    "    return output\n",
    "\n",
    "# Command-line interface\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python encode_images.py <config_path>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    config_path = sys.argv[1]\n",
    "    encode_images(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f5130-d62e-47db-82b1-5e075565d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_images(\"config.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
